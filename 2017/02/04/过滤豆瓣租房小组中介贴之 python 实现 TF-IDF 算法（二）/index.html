<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 过滤豆瓣租房小组中介贴之 python 实现 TF-IDF 算法（二） · 程序化思维</title><meta name="description" content="过滤豆瓣租房小组中介贴之 python 实现 TF-IDF 算法（二） - facert"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://facert.github.io/atom.xml" title="程序化思维"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/facert" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">过滤豆瓣租房小组中介贴之 python 实现 TF-IDF 算法（二）</h1><div class="post-info">Feb 4, 2017</div><div class="post-content"><p>前段时间写过一篇 <a href="https://facert.github.io/2016/11/24/%E8%BF%87%E6%BB%A4%E8%B1%86%E7%93%A3%E7%A7%9F%E6%88%BF%E5%B0%8F%E7%BB%84%E4%B8%AD%E4%BB%8B%E8%B4%B4%E4%B9%8B%20python%20%E5%AE%9E%E7%8E%B0%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%EF%BC%88%E4%B8%80%EF%BC%89/">过滤豆瓣租房小组中介贴之 python 实现余弦相似度（一）</a>, 这里面使用 <code>jieba</code> 将文档分词，然后计算对分词的结果向量化计算相似度。比如 <code>我的房子在方庄地铁附近的芳城园一区</code> 会被分词为 <code>方庄 芳城园 一区 地铁 房子 附近</code>，我们发现少了 <code>我 的 在</code> 这些词，在自然语言处理的过程中，称这些词为停用词，简单的说就是这些词对于分词结果没有多少帮助，所以需要直接过滤掉。因为我们直接调用 <code>jieba.analyse</code> 方法，所以停用词被直接过滤了，如果只是调用 <code>jieba.cut</code>，会将返回包含停用词的结果。<br><a id="more"></a><br>再回到刚才的例子， <code>方庄 芳城园 一区 地铁 房子 附近</code> 这个结果中到底哪个词需要重点关注，就是所谓的关键词。这个时候我们就引出了今天的主角，<code>TF-IDF 算法</code>， 具体原理可以参见这篇通俗易懂的文章，<a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2013/03/tf-idf.html</a> 。至于为什么一步步的得出这种算法，也可以参考数学之美第 11 章。</p>
<p>那么如何用 python 实现这种算法呢，看过原理的发现其实这个算法不难，我们发现 jieba 已经自带这个算法了， 基于 TF-IDF 算法的关键词抽取，代码示例： <a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py，" target="_blank" rel="noopener">https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py，</a> 试一下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">content = <span class="string">u'我的房子在方庄地铁附近的芳城园一区'</span></span><br><span class="line"><span class="keyword">for</span> i  <span class="keyword">in</span> jieba.analyse.extract_tags(content, topK=<span class="number">20</span>, withWeight=<span class="keyword">True</span>):</span><br><span class="line">    <span class="keyword">print</span> i</span><br></pre></td></tr></table></figure>
<p>结果如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(u&apos;\u65b9\u5e84&apos;, 2.0485399565833333)</span><br><span class="line">(u&apos;\u82b3\u57ce\u56ed&apos;, 1.9924612504833332)</span><br><span class="line">(u&apos;\u4e00\u533a&apos;, 1.7444484079166667)</span><br><span class="line">(u&apos;\u5730\u94c1&apos;, 1.3667056797650001)</span><br><span class="line">(u&apos;\u623f\u5b50&apos;, 1.0526507623933334)</span><br><span class="line">(u&apos;\u9644\u8fd1&apos;, 0.8615568819066667)</span><br><span class="line"></span><br><span class="line">方庄</span><br><span class="line">芳城园</span><br><span class="line">一区</span><br><span class="line">地铁</span><br><span class="line">房子</span><br><span class="line">附近</span><br></pre></td></tr></table></figure>
<p>对于租房子来说，地点确实最关键，所以 <code>方庄 芳城园 一区</code>三个词权重最高，然后是<code>地铁</code>，所以达到了想要的效果，当然只拿一个例子说明肯定是不够的。另外对于一些特殊的行业，有各自不同的术语，所以自定义语料很重要，比如租房的话，可以爬取所有租房帖子，然后分词，制作自己的语料库，最后得出的结果会更好。</p>
<p>自定义语料简单代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">all_dict = &#123;&#125;</span><br><span class="line">for line in lines:</span><br><span class="line">    temp_dict = &#123;&#125;</span><br><span class="line">    total += 1</span><br><span class="line">    cut_line = jieba.cut(line, cut_all=False)</span><br><span class="line">    for word in cut_line:</span><br><span class="line">        temp_dict[word] = 1</span><br><span class="line">    for key in temp_dict:</span><br><span class="line">        num = all_dict.get(key, 0)</span><br><span class="line">        all_dict[key] = num + 1</span><br><span class="line">for key in all_dict:</span><br><span class="line">    w = key.encode(&apos;utf-8&apos;)</span><br><span class="line">    p = &apos;%.10f&apos; % (math.log10(total/(all_dict[key] + 1)))</span><br></pre></td></tr></table></figure>
<p>代码引用自：<a href="https://github.com/fxsjy/jieba/issues/393" target="_blank" rel="noopener">https://github.com/fxsjy/jieba/issues/393</a></p>
<p>最后，<code>TF-IDF 算法</code> 应用非常广泛，比如你用搜索引擎搜索的时候，如何确定你一句话的搜索关键词是什么，它功不可没。</p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/03/06/过滤豆瓣租房小组中介贴之-python-实现-k-近邻算法（四）/" class="prev">上一篇</a><a href="/2017/02/04/过滤豆瓣租房小组中介贴之 python 实现 布隆过滤器（三）/" class="next">下一篇</a></div><div class="copyright"><p>© 2021 <a href="https://facert.github.io">facert</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>