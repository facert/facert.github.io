<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 如何通过 python 实现一个`正经` tumblr 爬虫 · 程序化思维</title><meta name="description" content="如何通过 python 实现一个`正经` tumblr 爬虫 - facert"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://facert.github.io/atom.xml" title="程序化思维"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/facert" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">如何通过 python 实现一个`正经` tumblr 爬虫</h1><div class="post-info">Nov 21, 2016</div><div class="post-content"><p>直接入正题，写一个 python 爬虫，最重要的其实是找到入口，从哪里开始爬，以 tumblr 为例，只要能够获取到用户列表，通过每个用户的主页就能拿到每个用户发的或者转发的视频，原作者的名字也可以拿到。<br><a id="more"></a><br>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(self, url)</span>:</span></span><br><span class="line">    <span class="comment"># 用 requests 库获取 url 的网页内容</span></span><br><span class="line">    res = requests.get(url)</span><br><span class="line">    <span class="comment"># 用 bs4 解析 html</span></span><br><span class="line">    soup = BeautifulSoup(res.text)  </span><br><span class="line">    <span class="comment"># 获取 iframe 标签列表</span></span><br><span class="line">    iframes = soup.find_all(<span class="string">'iframe'</span>)</span><br><span class="line">    tmp_source = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> iframes:</span><br><span class="line">        <span class="comment"># 获取视频地址</span></span><br><span class="line">        source = i.get(<span class="string">'src'</span>, <span class="string">''</span>).strip()</span><br><span class="line">        <span class="comment"># 过滤内容</span></span><br><span class="line">        <span class="keyword">if</span> source <span class="keyword">and</span> source.find(<span class="string">'https://www.tumblr.com/video'</span>) != <span class="number">-1</span> <span class="keyword">and</span> source <span class="keyword">not</span> <span class="keyword">in</span> self.total_url:</span><br><span class="line">            tmp_source.append(source)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">u'新增链接:'</span> + source</span><br><span class="line"></span><br><span class="line">    tmp_user = []</span><br><span class="line">    <span class="comment"># 获取用户列表</span></span><br><span class="line">    new_users = soup.find_all(class_=<span class="string">'reblog-link'</span>)</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> new_users:</span><br><span class="line">        username = user.text.strip()</span><br><span class="line">        <span class="comment"># 过滤用户</span></span><br><span class="line">        <span class="keyword">if</span> username <span class="keyword">and</span> username <span class="keyword">not</span> <span class="keyword">in</span> self.total_user:</span><br><span class="line">            <span class="comment"># 将用户放入待爬取队列</span></span><br><span class="line">            self.user_queue.put(username)</span><br><span class="line">            <span class="comment"># 加入总用户列表中</span></span><br><span class="line">            self.total_user.append(username)</span><br><span class="line">            tmp_user.append(username)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">u'新增用户:'</span> + username</span><br></pre></td></tr></table></figure>
<p>为了提高爬虫的性能，可以采用多线程的方式，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tumblr</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 为了能处理多线程 ctr+c 的信号问题，具体见完整代码</span></span><br><span class="line">        <span class="keyword">global</span> is_exit</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> is_exit:</span><br><span class="line">            user = self.user_queue.get()</span><br><span class="line">            url = <span class="string">'http://%s.tumblr.com/'</span> % user</span><br><span class="line">            self.download(url)</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 线程数</span></span><br><span class="line">    NUM_WORKERS = <span class="number">10</span></span><br><span class="line">    <span class="comment"># 待爬取队列</span></span><br><span class="line">    queue = Queue.Queue()</span><br><span class="line">    <span class="comment"># 开始的时候将一个热门博主的 username 加入到待爬取队列</span></span><br><span class="line">    queue.put(<span class="string">'username'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_WORKERS):</span><br><span class="line">        <span class="comment"># 实例化线程对象</span></span><br><span class="line">        tumblr = Tumblr(queue)</span><br><span class="line">        <span class="comment"># 设置线程为守护线程</span></span><br><span class="line">        tumblr.setDaemon(<span class="keyword">True</span>)</span><br><span class="line">        tumblr.start()</span><br></pre></td></tr></table></figure>
<p>最后将用户和视频资源写入到文件中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于多线程写文件会出现线程安全问题，所以加了个线程锁</span></span><br><span class="line">mutex.acquire()</span><br><span class="line"><span class="keyword">if</span> tmp_user:</span><br><span class="line">    self.f_user.write(<span class="string">'\n'</span>.join(tmp_user)+<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">if</span> tmp_source:</span><br><span class="line">    self.f_source.write(<span class="string">'\n'</span>.join(tmp_source)+<span class="string">'\n'</span>)</span><br><span class="line"><span class="comment"># 释放锁</span></span><br><span class="line">mutex.release()</span><br></pre></td></tr></table></figure>
<p>这样一个多线程 python 爬虫就基本完成了，完整代码见 <a href="https://github.com/facert/tumblr_spider" target="_blank" rel="noopener">github</a></p>
<p>最后声明这是一个正经的爬虫（严肃脸），爬取的资源跟你第一个填入的 username 有很大关系，老司机怎么开车我不管 ╭(╯^╰)╮</p>
</div></article></div></main><footer><div class="paginator"><a href="/2016/11/24/过滤豆瓣租房小组中介贴之 python 实现余弦相似度（一）/" class="prev">PREV</a><a href="/2016/11/19/微信找房机器人的实现/" class="next">NEXT</a></div><div class="copyright"><p>© 2021 <a href="https://facert.github.io">facert</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>