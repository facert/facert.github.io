<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 微信找房机器人 · 程序化思维</title><meta name="description" content="微信找房机器人 - facert"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://facert.github.io/atom.xml" title="程序化思维"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/facert" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">微信找房机器人</h1><div class="post-info">Nov 19, 2016</div><div class="post-content"><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>身在帝都的人都知道租房的困难，每次找房都是心力交瘁。其中豆瓣租房小组算是比较靠谱的房源了，但是由于小组信息繁杂，而且没有搜索的功能，想要实时获取租房信息是件很困难的事情，所以最近给自己挖了个坑，做个微信找房机器人，先看大概效果吧，见下图：<br><img src="/media/WechatIMG5.jpeg" alt="WechatIMG5"></p>
<a id="more"></a>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>说下大概的技术实现吧，首先是 scrapy 爬虫对于豆瓣北京租房的小组实时爬取，存储在 mongodb 中，因为做的是全文检索，所以对 title, description 使用 jieba 和 whoosh 进行了分词和索引，做成 api。接下来就是应用的接入，网上有微信机器人的开源 <a href="https://github.com/liuwons/wxBot" target="_blank" rel="noopener">wxBot</a>，所以对它进行了修改, 实现了定时推送和持久化。最后顺便把公众号也做了同样的功能，支持实时租房信息搜索。</p>
<h4 id="部分代码"><a href="#部分代码" class="headerlink" title="部分代码"></a>部分代码</h4><p>scrapy 支持自定义 pipeline，能很方便的实现数据录入的时候实时生成索引，见 code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IndexPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        self.index = index</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            index=crawler.settings.get(<span class="string">'WHOOSH_INDEX'</span>, <span class="string">'indexes'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.writer = AsyncWriter(get_index(self.index, zufang_schema))</span><br><span class="line">        create_time = datetime.datetime.strptime(item[<span class="string">'create_time'</span>], <span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">        self.writer.update_document(</span><br><span class="line">            url=item[<span class="string">'url'</span>].decode(<span class="string">'utf-8'</span>),</span><br><span class="line">            title=item[<span class="string">'title'</span>],</span><br><span class="line">            description=item[<span class="string">'description'</span>],</span><br><span class="line">            create_time=create_time</span><br><span class="line">        )</span><br><span class="line">        self.writer.commit()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>搜索 api 代码很简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zufang_query</span><span class="params">(keywords, limit=<span class="number">100</span>)</span>:</span></span><br><span class="line">    ix = get_index(<span class="string">'indexes'</span>, zufang_schema)</span><br><span class="line">    content = [<span class="string">"title"</span>, <span class="string">"description"</span>]</span><br><span class="line">    query = MultifieldParser(content, ix.schema).parse(keywords)</span><br><span class="line"></span><br><span class="line">    result_list = []</span><br><span class="line">    <span class="keyword">with</span> ix.searcher() <span class="keyword">as</span> searcher:</span><br><span class="line">        results = searcher.search(query, sortedby=<span class="string">"create_time"</span>, reverse=<span class="keyword">True</span>, limit=limit)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> results:</span><br><span class="line">            result_list.append(&#123;<span class="string">'url'</span>: i[<span class="string">'url'</span>], <span class="string">'title'</span>: i[<span class="string">'title'</span>], <span class="string">'create_time'</span>: i[<span class="string">'create_time'</span>]&#125;)</span><br><span class="line">    <span class="keyword">return</span> result_list</span><br></pre></td></tr></table></figure>
<p>微信机器人代码自己去看原作者的吧，写的挺清晰的</p>
<p>至于整个项目的代码现在还没整理完，整理完后考虑开源</p>
<h4 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h4><p>大家有兴趣的话可以试用下机器人，搜索微信号 jeeeeeffer 或者扫描二维码，当然公众号也有同样的功能，搜索【嗅房】就行。<br><img src="/media/WechatIMG8.jpeg" alt="WechatIMG8"></p>
<p>最后建了个微信机器人找房交流群，你可以在群里面直接 @Flash 搜索房源或者一起交流技术。<br><img src="/media/WechatIMG7.jpeg" alt="WechatIMG7"></p>
</div></article></div></main><footer><div class="paginator"><a href="/2016/11/21/如何通过-python-实现一个-正经-tumblr-爬虫/" class="prev">上一篇</a><a href="/2016/10/13/python-与-数据结构-第三章-线性表-（中）/" class="next">下一篇</a></div><div class="copyright"><p>© 2021 <a href="https://facert.github.io">facert</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>